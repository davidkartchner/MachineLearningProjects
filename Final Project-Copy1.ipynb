{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from biometric_helper import *\n",
    "from numpy.random import shuffle, choice\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "biometric_helper.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  group[\"Samplerate\"] = 50\n"
     ]
    }
   ],
   "source": [
    "accel_data = clean_data(pd.read_csv(\"ProjectData.csv\"))\n",
    "accel_data.JointID /= accel_data.JointID.max()\n",
    "accel_data.JointID -= accel_data.JointID.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# accel_data.describe()\n",
    "batch_size=100\n",
    "sample_time = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 150)\n",
      "(100, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidspencerkartchner/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:24: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# def train_test_split(feats, labels, ratio=.7):\n",
    "    \n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self, data, rate, b_size=100, time=3):\n",
    "        train_samples = []\n",
    "        train_labels = []\n",
    "        test_samples = []\n",
    "        test_labels = []\n",
    "        \n",
    "        seconds = 3\n",
    "        chunksize = int(rate*seconds)\n",
    "        self.chunksize = chunksize\n",
    "        \n",
    "        # Split the data into 3 second segments\n",
    "        grouped = data.groupby(\"JointID\")\n",
    "        for person in grouped.groups:\n",
    "            temp = grouped.get_group(person)\n",
    "            n_samples = temp.shape[0]//chunksize\n",
    "            feats = temp.loc[:,\"A_tot\"].as_matrix()\n",
    "            labels = np.ones(n_samples)*temp.JointID.as_matrix()[0]\n",
    "            samples = np.array([feats[chunksize*n:chunksize*(n+1)].flatten()\\\n",
    "                                for n in xrange(n_samples)])\n",
    "            \n",
    "            test_rows = choice(n_samples, size=np.floor(.3*n_samples), replace=False)\n",
    "            test_samples.append(samples[test_rows])\n",
    "            test_labels.append(labels[test_rows])\n",
    "            train_mask = np.ones(n_samples, dtype=bool)\n",
    "            train_mask[test_rows] = 0\n",
    "            train_samples.append(samples[train_mask])\n",
    "            train_labels.append(labels[train_mask])\n",
    "        \n",
    "        self.train_samples = np.vstack(tuple(train_samples))\n",
    "        self.test_samples = np.vstack(tuple(test_samples))\n",
    "        \n",
    "        self.train_size = self.train_samples.shape[0]\n",
    "        self.test_size = self.test_samples.shape[0]\n",
    "        \n",
    "        self.train_labels = np.hstack(tuple(train_labels)).flatten().reshape((self.train_size,1))\n",
    "        self.train_labels = pd.get_dummies(pd.DataFrame(self.train_labels), columns=[0]).as_matrix()\n",
    "\n",
    "        self.test_labels = np.hstack(tuple(test_labels)).flatten().reshape((self.test_size,1))\n",
    "        self.test_labels = pd.get_dummies(pd.DataFrame(self.test_labels), columns=[0]).as_matrix()\n",
    "        \n",
    "        self.train_iter = 0\n",
    "        self.test_iter = 0\n",
    "        \n",
    "        self.b_size = b_size\n",
    "        self.n_train_batches = self.train_size//self.b_size\n",
    "        self.n_test_batches = self.test_size//self.b_size\n",
    "        \n",
    "        \n",
    "    def reset_batches(self):\n",
    "        merged_train = np.hstack((self.train_samples, self.train_labels))\n",
    "        shuffle(merged_train)\n",
    "        merged_test = np.hstack((self.test_samples, self.test_labels))\n",
    "        shuffle(merged_test)\n",
    "        \n",
    "        self.train_iter = 0\n",
    "        self.test_iter = 0\n",
    "        \n",
    "        self.train_samples = merged_train[:,:self.chunksize]\n",
    "        self.train_labels = merged_train[:,self.chunksize:]\n",
    "        self.test_samples = merged_test[:,:self.chunksize]\n",
    "        self.test_labels = merged_test[:,self.chunksize:]\n",
    "#         print self.train_samples.shape\n",
    "#         print self.train_labels.shape\n",
    "#         print self.train_labels[:5]\n",
    "        \n",
    "    def train_batch(self):\n",
    "        a = self.train_iter*self.b_size\n",
    "        b = (self.train_iter+1)*self.b_size\n",
    "        self.train_iter += 1\n",
    "        return self.train_samples[a:b], self.train_labels[a:b] \n",
    "    \n",
    "    \n",
    "    def test_batch(self):\n",
    "        a = self.test_iter*self.b_size\n",
    "        b = (self.test_iter+1)*self.b_size\n",
    "        self.test_iter += 1\n",
    "        return self.test_samples[a:b], self.test_labels[a:b]\n",
    "    \n",
    "    def num_train_batches(self):\n",
    "        return self.n_train_batches\n",
    "    \n",
    "    def num_test_batches(self):\n",
    "        return self.n_test_batches\n",
    "    \n",
    "        \n",
    "loader = DataLoader(accel_data,50, b_size=batch_size, time=sample_time)\n",
    "loader.reset_batches()\n",
    "j, k = loader.test_batch()\n",
    "print j.shape\n",
    "print k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def conv1d( in_var, output_dim, width=10, stride=1, name=\"conv1d\" ):\n",
    "    k_x = width  # filter width\n",
    "    d_x = stride  # x stride\n",
    "\n",
    "    with tf.variable_scope( name ):\n",
    "        W = tf.get_variable( \"W\", [k_x, in_var.get_shape()[-1], output_dim],\n",
    "                             initializer=tf.truncated_normal_initializer(stddev=0.02) )\n",
    "        b = tf.get_variable( \"b\", [output_dim], initializer=tf.constant_initializer(0.01) )\n",
    "\n",
    "        conv = tf.nn.conv1d( in_var, W, stride=stride, padding='SAME' )\n",
    "        conv = tf.reshape( tf.nn.bias_add( conv, b ), conv.get_shape() )\n",
    "\n",
    "        return conv\n",
    "\n",
    "    \n",
    "def linear( in_var, output_size, name=\"linear\", stddev=0.02, bias_val=.01 ):\n",
    "    shape = in_var.get_shape().as_list()\n",
    "\n",
    "    with tf.variable_scope( name ):\n",
    "        W = tf.get_variable( \"W\", [shape[1], output_size], tf.float32,\n",
    "                              tf.random_normal_initializer( stddev=stddev ) )\n",
    "        b = tf.get_variable( \"b\", [output_size],\n",
    "                             initializer=tf.constant_initializer( bias_val ))\n",
    "\n",
    "        return tf.matmul( in_var, W ) + b    \n",
    "    \n",
    "    \n",
    "    \n",
    "def conv_resnet(in_var, out_shape, stride=1, width=5, name='resnet'):\n",
    "    \"\"\"\n",
    "    Make a ResNet layer, i.e. two convolutions and an identity mapping\n",
    "    \"\"\"\n",
    "    x_stride = stride\n",
    "    std = .02\n",
    "    const = .01\n",
    "    \n",
    "    if stride!=1 and stride!=2:\n",
    "        print stride\n",
    "        raise ValueError(\"Stride must be 1 or 2\")\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        # First convolution in ResNet layer\n",
    "        conv1 = conv1d(in_var, out_shape, stride=stride, width=width)\n",
    "        \n",
    "        # Relu-fy the convolutional layer\n",
    "        relu_layer = tf.nn.relu(conv1)\n",
    "        \n",
    "        # Second convolution in ResNet layer\n",
    "        conv2 = conv1d(in_var, out_shape, width=width)\n",
    "        \n",
    "        if stride==1:\n",
    "            ident= in_var\n",
    "#         else:\n",
    "#             w3 = tf.get_variable(\"W3\", [1, 1, in_var.get_shape()[-1], out_shape], \n",
    "#                              initializer=tf.truncated_normal_initializer(stddev=std))\n",
    "#             in_var = tf.nn.conv1d(in_var, w3, stride=1 padding=\"SAME\")\n",
    "#             in_var = tf.reshape(in_var)\n",
    "#             ident = tf.nn.max_pool(in_var, ksize=[1,2, 1, 1],\n",
    "#                         strides=[1, 2, 2, 1], padding='SAME')\n",
    "# #             print in_var.get_shape()\n",
    "        \n",
    "#         # Add identity and run through another relu\n",
    "        add_x = tf.nn.relu(conv2 + ident)\n",
    "    return add_x\n",
    "        \n",
    "        \n",
    "# def siamese_block(imgs):\n",
    "#     imgs = tf.reshape(imgs, [batch_size, x_dim, x_dim, 1])\n",
    "#     imgs = tf.nn.avg_pool(imgs,[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "#     conv0 = conv2d(imgs, 64, name='first_conv')\n",
    "#     res1 = resnet_block(conv0, 64, name='res1')\n",
    "#     res2 = resnet_block(res1, 64, name='res2')\n",
    "#     res3 = resnet_block(res2, 128, stride=2, name='res3')\n",
    "#     res4 = resnet_block(res3, 128, name='res4')\n",
    "#     res5 = resnet_block(res4, 256, stride=2, name='res5')\n",
    "#     pool1 = tf.reshape(tf.nn.max_pool(res5, [1,2,2,1], strides=[1,2,2,1], padding='SAME'), [batch_size,-1])\n",
    "#     fc_1 = linear(pool1, 1024, name='fc1')\n",
    "#     return fc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [batch_size, 50*sample_time])\n",
    "y = tf.placeholder(tf.float32, [batch_size, 42])\n",
    "\n",
    "fc1 = tf.reshape(linear(x, 128, name=\"fc_1\"), [batch_size, 128, 1])\n",
    "relu1 = tf.nn.relu(fc1)\n",
    "conv2 = conv1d(relu1, 64, name='conv2')\n",
    "relu2 = tf.nn.relu(conv2)\n",
    "conv3 = conv1d(relu2, 256, width=3, name='conv3')\n",
    "relu3 = tf.nn.relu(conv3)\n",
    "fc4 = linear(tf.reshape(relu3, [batch_size, -1]), 512, name='fc_4')\n",
    "fc5 = linear(fc4, 42, name='fc_5')\n",
    "\n",
    "score = tf.log(tf.nn.softmax(fc5))\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(score * y))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(score,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "optim = tf.train.AdamOptimizer(1e-7).minimize(loss)\n",
    "\n",
    "\n",
    "\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0 Loss: 586.975\n",
      "0 Test Accuracy: 0.0305556\n",
      "1\n",
      "1 Loss: 517.186\n",
      "2\n",
      "2 Loss: 469.578\n",
      "3\n",
      "3 Loss: 437.285\n",
      "4\n",
      "4 Loss: 412.072\n",
      "5\n",
      "5 Loss: 388.666\n",
      "6\n",
      "6 Loss: 366.791\n",
      "7\n",
      "7 Loss: 346.813\n",
      "8\n",
      "8 Loss: 328.468\n",
      "9\n",
      "9 Loss: 312.94\n",
      "10\n",
      "10 Loss: 300.195\n",
      "10 Test Accuracy: 0.386667\n",
      "11\n",
      "11 Loss: 290.143\n",
      "12\n",
      "12 Loss: 282.435\n",
      "13\n",
      "13 Loss: 276.314\n",
      "14\n",
      "14 Loss: 271.357\n",
      "15\n",
      "15 Loss: 267.159\n",
      "16\n",
      "16 Loss: 263.647\n",
      "17\n",
      "17 Loss: 260.846\n",
      "18\n",
      "18 Loss: 258.19\n",
      "19\n",
      "19 Loss: 256.138\n",
      "20\n",
      "20 Loss: 254.197\n",
      "20 Test Accuracy: 0.4\n",
      "21\n",
      "21 Loss: 252.803\n",
      "22\n",
      "22 Loss: 251.48\n",
      "23\n",
      "23 Loss: 250.238\n",
      "24\n",
      "24 Loss: 249.304\n",
      "25\n",
      "25 Loss: 248.401\n",
      "26\n",
      "26 Loss: 247.407\n",
      "27\n",
      "27 Loss: 246.462\n",
      "28\n",
      "28 Loss: 245.656\n",
      "29\n",
      "29 Loss: 244.915\n",
      "30\n",
      "30 Loss: 244.137\n",
      "30 Test Accuracy: 0.418333\n",
      "31\n",
      "31 Loss: 243.062\n",
      "32\n",
      "32 Loss: 242.609\n",
      "33\n",
      "33 Loss: 241.69\n",
      "34\n",
      "34 Loss: 241.176\n",
      "35\n",
      "35 Loss: 240.516\n",
      "36\n",
      "36 Loss: 239.877\n",
      "37\n",
      "37 Loss: 239.132\n",
      "38\n",
      "38 Loss: 238.558\n",
      "39\n",
      "39 Loss: 238.015\n",
      "40\n",
      "40 Loss: 237.381\n",
      "40 Test Accuracy: 0.408889\n",
      "41\n",
      "41 Loss: 236.811\n",
      "42\n",
      "42 Loss: 236.163\n",
      "43\n",
      "43 Loss: 235.563\n",
      "44\n",
      "44 Loss: 235.124\n",
      "45\n",
      "45 Loss: 234.582\n",
      "46\n",
      "46 Loss: 234.058\n",
      "47\n",
      "47 Loss: 233.556\n",
      "48\n",
      "48 Loss: 233.098\n",
      "49\n",
      "49 Loss: 232.6\n",
      "50\n",
      "50 Loss: 232.068\n",
      "50 Test Accuracy: 0.429444\n",
      "51\n",
      "51 Loss: 231.728\n",
      "52\n",
      "52 Loss: 231.311\n",
      "53\n",
      "53 Loss: 230.884\n",
      "54\n",
      "54 Loss: 230.389\n",
      "55\n",
      "55 Loss: 229.98\n",
      "56\n",
      "56 Loss: 229.648\n",
      "57\n",
      "57 Loss: 229.157\n",
      "58\n",
      "58 Loss: 229.046\n",
      "59\n",
      "59 Loss: 228.708\n",
      "60\n",
      "60 Loss: 228.338\n",
      "60 Test Accuracy: 0.451111\n",
      "61\n",
      "61 Loss: 228.144\n",
      "62\n",
      "62 Loss: 227.868\n",
      "63\n",
      "63 Loss: 227.48\n",
      "64\n",
      "64 Loss: 227.156\n",
      "65\n",
      "65 Loss: 226.992\n",
      "66\n",
      "66 Loss: 226.671\n",
      "67\n",
      "67 Loss: 226.459\n",
      "68\n",
      "68 Loss: 226.188\n",
      "69\n",
      "69 Loss: 226.003\n",
      "70\n",
      "70 Loss: 225.693\n",
      "70 Test Accuracy: 0.459444\n",
      "71\n",
      "71 Loss: 225.627\n",
      "72\n",
      "72 Loss: 225.342\n",
      "73\n",
      "73 Loss: 225.142\n",
      "74\n",
      "74 Loss: 224.97\n",
      "75\n",
      "75 Loss: 224.542\n",
      "76\n",
      "76 Loss: 224.395\n",
      "77\n",
      "77 Loss: 224.375\n",
      "78\n",
      "78 Loss: 224.135\n",
      "79\n",
      "79 Loss: 223.962\n",
      "80\n",
      "80 Loss: 223.588\n",
      "80 Test Accuracy: 0.464444\n",
      "81\n",
      "81 Loss: 223.459\n",
      "82\n",
      "82 Loss: 223.22\n",
      "83\n",
      "83 Loss: 223.114\n",
      "84\n",
      "84 Loss: 222.896\n",
      "85\n",
      "85 Loss: 222.747\n",
      "86\n",
      "86 Loss: 222.565\n",
      "87\n",
      "87 Loss: 222.376\n",
      "88\n",
      "88 Loss: 222.22\n",
      "89\n",
      "89 Loss: 222.004\n",
      "90\n",
      "90 Loss: 221.798\n",
      "90 Test Accuracy: 0.462222\n",
      "91\n",
      "91 Loss: 221.754\n",
      "92\n",
      "92 Loss: 221.45\n",
      "93\n",
      "93 Loss: 221.29\n",
      "94\n",
      "94 Loss: 221.107\n",
      "95\n",
      "95 Loss: 220.992\n",
      "96\n",
      "96 Loss: 220.937\n",
      "97\n",
      "97 Loss: 220.657\n",
      "98\n",
      "98 Loss: 220.452\n",
      "99\n",
      "99 Loss: 220.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidspencerkartchner/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:24: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(accel_data,50, b_size=batch_size, time=sample_time)\n",
    "\n",
    "num_train = loader.num_train_batches()\n",
    "num_test = loader.num_test_batches()\n",
    "loss_vals = []\n",
    "for i in xrange(100):\n",
    "    loader.reset_batches()\n",
    "    step_loss = []\n",
    "    for j in xrange(num_train):\n",
    "        xs, ys = loader.train_batch()\n",
    "        sess.run(optim, feed_dict={x:xs, y:ys})\n",
    "        l = sess.run(loss, feed_dict={x:xs, y:ys})\n",
    "        step_loss.append(l)\n",
    "    loss_vals.append(np.mean(step_loss))\n",
    "    print i, \"Loss:\", loss_vals[-1]\n",
    "        \n",
    "    \n",
    "    if i%10==0:\n",
    "        acc_scores = []\n",
    "        for k in xrange(num_test):\n",
    "            loader.reset_batches()\n",
    "            xs, ys = loader.test_batch()\n",
    "            acc = sess.run(accuracy, feed_dict={x:xs, y:ys})\n",
    "            acc_scores.append(acc)\n",
    "        test_acc = np.mean(acc_scores)\n",
    "        print i, \"Test Accuracy:\", test_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
