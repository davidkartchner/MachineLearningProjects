{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from biometric_helper import *\n",
    "from numpy.random import shuffle, choice\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "biometric_helper.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  group[\"Samplerate\"] = 50\n"
     ]
    }
   ],
   "source": [
    "accel_data = clean_data(pd.read_csv(\"ProjectData.csv\"))\n",
    "accel_data.A_tot /= accel_data.A_tot.max()\n",
    "accel_data.A_tot -= accel_data.A_tot.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# accel_data.describe()\n",
    "batch_size=100\n",
    "sample_time = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 150)\n",
      "(100, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:24: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# def train_test_split(feats, labels, ratio=.7):\n",
    "    \n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self, data, rate, b_size=100, time=3):\n",
    "        train_samples = []\n",
    "        train_labels = []\n",
    "        test_samples = []\n",
    "        test_labels = []\n",
    "        \n",
    "        seconds = 3\n",
    "        chunksize = int(rate*seconds)\n",
    "        self.chunksize = chunksize\n",
    "        \n",
    "        # Split the data into 3 second segments\n",
    "        grouped = data.groupby(\"JointID\")\n",
    "        for person in grouped.groups:\n",
    "            temp = grouped.get_group(person)\n",
    "            n_samples = temp.shape[0]//chunksize\n",
    "            feats = temp.loc[:,\"A_tot\"].as_matrix()\n",
    "            labels = np.ones(n_samples)*temp.JointID.as_matrix()[0]\n",
    "            samples = np.array([feats[chunksize*n:chunksize*(n+1)].flatten()\\\n",
    "                                for n in xrange(n_samples)])\n",
    "            \n",
    "            test_rows = choice(n_samples, size=np.floor(.3*n_samples), replace=False)\n",
    "            test_samples.append(samples[test_rows])\n",
    "            test_labels.append(labels[test_rows])\n",
    "            train_mask = np.ones(n_samples, dtype=bool)\n",
    "            train_mask[test_rows] = 0\n",
    "            train_samples.append(samples[train_mask])\n",
    "            train_labels.append(labels[train_mask])\n",
    "        \n",
    "        self.train_samples = np.vstack(tuple(train_samples))\n",
    "        self.test_samples = np.vstack(tuple(test_samples))\n",
    "        \n",
    "        self.train_size = self.train_samples.shape[0]\n",
    "        self.test_size = self.test_samples.shape[0]\n",
    "        \n",
    "        self.train_labels = np.hstack(tuple(train_labels)).flatten().reshape((self.train_size,1))\n",
    "        self.train_labels = pd.get_dummies(pd.DataFrame(self.train_labels), columns=[0]).as_matrix()\n",
    "\n",
    "        self.test_labels = np.hstack(tuple(test_labels)).flatten().reshape((self.test_size,1))\n",
    "        self.test_labels = pd.get_dummies(pd.DataFrame(self.test_labels), columns=[0]).as_matrix()\n",
    "        \n",
    "        self.train_iter = 0\n",
    "        self.test_iter = 0\n",
    "        \n",
    "        self.b_size = b_size\n",
    "        self.n_train_batches = self.train_size//self.b_size\n",
    "        self.n_test_batches = self.test_size//self.b_size\n",
    "        \n",
    "        \n",
    "    def reset_batches(self):\n",
    "        merged_train = np.hstack((self.train_samples, self.train_labels))\n",
    "        shuffle(merged_train)\n",
    "        merged_test = np.hstack((self.test_samples, self.test_labels))\n",
    "        shuffle(merged_test)\n",
    "        \n",
    "        self.train_iter = 0\n",
    "        self.test_iter = 0\n",
    "        \n",
    "        self.train_samples = merged_train[:,:self.chunksize]\n",
    "        self.train_labels = merged_train[:,self.chunksize:]\n",
    "        self.test_samples = merged_test[:,:self.chunksize]\n",
    "        self.test_labels = merged_test[:,self.chunksize:]\n",
    "#         print self.train_samples.shape\n",
    "#         print self.train_labels.shape\n",
    "#         print self.train_labels[:5]\n",
    "        \n",
    "    def train_batch(self):\n",
    "        a = self.train_iter*self.b_size\n",
    "        b = (self.train_iter+1)*self.b_size\n",
    "        self.train_iter += 1\n",
    "        return self.train_samples[a:b], self.train_labels[a:b] \n",
    "    \n",
    "    \n",
    "    def test_batch(self):\n",
    "        a = self.test_iter*self.b_size\n",
    "        b = (self.test_iter+1)*self.b_size\n",
    "        self.test_iter += 1\n",
    "        return self.test_samples[a:b], self.test_labels[a:b]\n",
    "    \n",
    "    def num_train_batches(self):\n",
    "        return self.n_train_batches\n",
    "    \n",
    "    def num_test_batches(self):\n",
    "        return self.n_test_batches\n",
    "    \n",
    "        \n",
    "loader = DataLoader(accel_data,50, b_size=batch_size, time=sample_time)\n",
    "loader.reset_batches()\n",
    "j, k = loader.test_batch()\n",
    "print j.shape\n",
    "print k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def conv1d( in_var, output_dim, width=10, stride=1, name=\"conv1d\" ):\n",
    "    k_x = width  # filter width\n",
    "    d_x = stride  # x stride\n",
    "\n",
    "    with tf.variable_scope( name ):\n",
    "        W = tf.get_variable( \"W\", [k_x, in_var.get_shape()[-1], output_dim],\n",
    "                             initializer=tf.truncated_normal_initializer(stddev=0.02) )\n",
    "        b = tf.get_variable( \"b\", [output_dim], initializer=tf.constant_initializer(0.01) )\n",
    "\n",
    "        conv = tf.nn.conv1d( in_var, W, stride=stride, padding='SAME' )\n",
    "        conv = tf.reshape( tf.nn.bias_add( conv, b ), conv.get_shape() )\n",
    "\n",
    "        return conv\n",
    "\n",
    "    \n",
    "def linear( in_var, output_size, name=\"linear\", stddev=0.02, bias_val=.01 ):\n",
    "    shape = in_var.get_shape().as_list()\n",
    "\n",
    "    with tf.variable_scope( name ):\n",
    "        W = tf.get_variable( \"W\", [shape[1], output_size], tf.float32,\n",
    "                              tf.random_normal_initializer( stddev=stddev ) )\n",
    "        b = tf.get_variable( \"b\", [output_size],\n",
    "                             initializer=tf.constant_initializer( bias_val ))\n",
    "\n",
    "        return tf.matmul( in_var, W ) + b    \n",
    "    \n",
    "    \n",
    "    \n",
    "def conv_resnet(in_var, out_shape, stride=1, width=5, name='resnet'):\n",
    "    \"\"\"\n",
    "    Make a ResNet layer, i.e. two convolutions and an identity mapping\n",
    "    \"\"\"\n",
    "    x_stride = stride\n",
    "    std = .02\n",
    "    const = .01\n",
    "    \n",
    "    if stride!=1 and stride!=2:\n",
    "        print stride\n",
    "        raise ValueError(\"Stride must be 1 or 2\")\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        # First convolution in ResNet layer\n",
    "        conv1 = conv1d(in_var, out_shape, stride=stride, width=width)\n",
    "        \n",
    "        # Relu-fy the convolutional layer\n",
    "        relu_layer = tf.nn.relu(conv1)\n",
    "        \n",
    "        # Second convolution in ResNet layer\n",
    "        conv2 = conv1d(in_var, out_shape, width=width)\n",
    "        \n",
    "        if stride==1:\n",
    "            ident= in_var\n",
    "#         else:\n",
    "#             w3 = tf.get_variable(\"W3\", [1, 1, in_var.get_shape()[-1], out_shape], \n",
    "#                              initializer=tf.truncated_normal_initializer(stddev=std))\n",
    "#             in_var = tf.nn.conv1d(in_var, w3, stride=1 padding=\"SAME\")\n",
    "#             in_var = tf.reshape(in_var)\n",
    "#             ident = tf.nn.max_pool(in_var, ksize=[1,2, 1, 1],\n",
    "#                         strides=[1, 2, 2, 1], padding='SAME')\n",
    "# #             print in_var.get_shape()\n",
    "        \n",
    "#         # Add identity and run through another relu\n",
    "        add_x = tf.nn.relu(conv2 + ident)\n",
    "    return add_x\n",
    "        \n",
    "        \n",
    "# def siamese_block(imgs):\n",
    "#     imgs = tf.reshape(imgs, [batch_size, x_dim, x_dim, 1])\n",
    "#     imgs = tf.nn.avg_pool(imgs,[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "#     conv0 = conv2d(imgs, 64, name='first_conv')\n",
    "#     res1 = resnet_block(conv0, 64, name='res1')\n",
    "#     res2 = resnet_block(res1, 64, name='res2')\n",
    "#     res3 = resnet_block(res2, 128, stride=2, name='res3')\n",
    "#     res4 = resnet_block(res3, 128, name='res4')\n",
    "#     res5 = resnet_block(res4, 256, stride=2, name='res5')\n",
    "#     pool1 = tf.reshape(tf.nn.max_pool(res5, [1,2,2,1], strides=[1,2,2,1], padding='SAME'), [batch_size,-1])\n",
    "#     fc_1 = linear(pool1, 1024, name='fc1')\n",
    "#     return fc_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## First Architecture--Convnet experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [batch_size, 50*sample_time], name=\"x\")\n",
    "y = tf.placeholder(tf.float32, [batch_size, 42], name=\"y\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"FC_1\"):\n",
    "    fc1 = tf.reshape(linear(x, 256, name=\"fc_1\"), [batch_size, 256, 1])\n",
    "    relu1 = tf.nn.relu(fc1)\n",
    "    \n",
    "with tf.name_scope(\"Conv_2\"):\n",
    "    conv2 = conv1d(relu1, 64, name='conv2')\n",
    "    relu2 = tf.nn.relu(conv2)\n",
    "    \n",
    "with tf.name_scope(\"FC_3\"):\n",
    "    fc4 = linear(tf.reshape(relu2, [batch_size, -1]), 1024, name='fc_3')\n",
    "    relu4 = tf.nn.relu(fc4)\n",
    "\n",
    "with tf.name_scope(\"Score\"):\n",
    "    fc5 = linear(relu4, 42, name='fc_4')\n",
    "    score = tf.log(tf.nn.softmax(fc5))\n",
    "\n",
    "with tf.name_scope(\"Loss\"):\n",
    "    # l2_regularization = tf.reduce_sum([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(score * y)) #+ .01*l2_regularization\n",
    "\n",
    "with tf.name_scope(\"Accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(score,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Optim\"):\n",
    "    optim = tf.train.AdamOptimizer(1e-5).minimize(loss)\n",
    "\n",
    "\n",
    "\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:24: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 360.28 0.418837\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(accel_data,50, b_size=batch_size, time=sample_time)\n",
    "\n",
    "num_train = loader.num_train_batches()\n",
    "num_test = loader.num_test_batches()\n",
    "loss_vals = []\n",
    "train_acc = []\n",
    "summary_writer = tf.train.SummaryWriter( \"./final_logs\", graph=sess.graph )\n",
    "for i in xrange(1,2):\n",
    "    loader.reset_batches()\n",
    "    step_loss = []\n",
    "    \n",
    "    temp_acc = []\n",
    "    for j in xrange(num_train):\n",
    "        xs, ys = loader.train_batch()\n",
    "        l = sess.run(loss, feed_dict={x:xs, y:ys})\n",
    "        acc = sess.run(accuracy, feed_dict={x:xs, y:ys})\n",
    "        sess.run(optim, feed_dict={x:xs, y:ys})\n",
    "        \n",
    "        step_loss.append(l)\n",
    "        temp_acc.append(acc)\n",
    "    loss_vals.append(np.mean(step_loss))\n",
    "    train_acc.append(np.mean(temp_acc))\n",
    "    print i, np.mean(loss_vals), np.mean(temp_acc)\n",
    "        \n",
    "    \n",
    "    if i%10==0:\n",
    "        acc_scores = []\n",
    "        for k in xrange(num_test):\n",
    "            loader.reset_batches()\n",
    "            xs, ys = loader.test_batch()\n",
    "            acc = sess.run(accuracy, feed_dict={x:xs, y:ys})\n",
    "            acc_scores.append(acc)\n",
    "        test_acc = np.mean(acc_scores)\n",
    "        print i, \"Test Accuracy:\", test_acc\n",
    "        print sess.run(fc5, feed_dict={x:xs, y:ys})\n",
    "        \n",
    "summary_writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Second Architecture: Fully Connected Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [batch_size, 50*sample_time])\n",
    "y = tf.placeholder(tf.float32, [batch_size, 42])\n",
    "\n",
    "fc1 = tf.reshape(linear(x, 600, name=\"fc_1\"), [batch_size, -1])\n",
    "relu1 = tf.nn.relu(fc1)\n",
    "\n",
    "fc4 = linear(tf.reshape(relu1, [batch_size, -1]), 600, name='fc_4')\n",
    "drop = tf.nn.dropout(fc4, .5)\n",
    "\n",
    "fc5 = linear(drop, 42, name='fc_5')\n",
    "\n",
    "l2_regularization = tf.reduce_mean([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
    "\n",
    "score = tf.log(tf.nn.softmax(fc5))\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(score * y) + 1e-5*l2_regularization)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(score,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "optim = tf.train.AdamOptimizer(1e-5).minimize(loss)\n",
    "\n",
    "\n",
    "\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "sess.run(fc1, feed_dict={x:xs, y:ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data and train network\n",
    "loader = DataLoader(accel_data,50, b_size=batch_size, time=sample_time)\n",
    "\n",
    "num_train = loader.num_train_batches()\n",
    "num_test = loader.num_test_batches()\n",
    "loss_vals = []\n",
    "for i in xrange(1000):\n",
    "    loader.reset_batches()\n",
    "    step_loss = []\n",
    "    for j in xrange(num_train):\n",
    "        xs, ys = loader.train_batch()\n",
    "        sess.run(optim, feed_dict={x:xs, y:ys})\n",
    "        l = sess.run(loss, feed_dict={x:xs, y:ys})\n",
    "        step_loss.append(l)\n",
    "    loss_vals.append(np.mean(step_loss))\n",
    "    print i, loss_vals[-1]\n",
    "        \n",
    "    \n",
    "    if i%10==0:\n",
    "        acc_scores = []\n",
    "        for k in xrange(num_test):\n",
    "            loader.reset_batches()\n",
    "            xs, ys = loader.test_batch()\n",
    "            acc = sess.run(accuracy, feed_dict={x:xs, y:ys})\n",
    "            acc_scores.append(acc)\n",
    "        test_acc = np.mean(acc_scores)\n",
    "        print i, \"Test Accuracy:\", test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "loader = DataLoader(accel_data,50, b_size=batch_size, time=sample_time)\n",
    "\n",
    "num_train = loader.num_train_batches()\n",
    "num_test = loader.num_test_batches()\n",
    "loss_vals = []\n",
    "# for i in xrange(100):\n",
    "loader.reset_batches()\n",
    "step_loss = []\n",
    "for j in xrange(num_train):\n",
    "    xs, ys = loader.train_batch()\n",
    "#         sess.run(optim, feed_dict={x:xs, y:ys})\n",
    "    l = sess.run(score, feed_dict={x:xs, y:ys})\n",
    "    print l\n",
    "    step_loss.append(l)\n",
    "loss_vals.append(np.mean(step_loss))\n",
    "print i, loss_vals[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
