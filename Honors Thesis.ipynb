{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Honors Thesis\n",
    "## An Exploratory Study of Biometric Gait Recognition\n",
    "## Compiled by David Kartchner\n",
    "## November 30, 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from __future__ import division\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import pywt\n",
    "from scipy.fftpack import fft, ifft\n",
    "from scipy.interpolate import interp1d\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (14.0, 7.0)\n",
    "pd.set_option('expand_frame_repr', True)\n",
    "pd.set_option('max_rows',200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Define some useful helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def get_rid_of_useless_data(data, col=\"A_tot\", trim_ends=False):\n",
    "    data = data.copy().reset_index()\n",
    "    rate = int(data.Samplerate[0])\n",
    "    \n",
    "    # Smooth data by taking wavelet transform\n",
    "    data[\"smoothed\"] = data[\"A_tot\"]#wavelet_smooth(data, [col])\n",
    "    \n",
    "    #Find standard deviation for 1 second rolling windows of smoothed data\n",
    "    new_col = col+\"_rolling_std\"\n",
    "    data[new_col] = data['smoothed'].rolling(center=True, \n",
    "                                             window=int(rate), \n",
    "                                             min_periods=rate//2).std()\n",
    "    \n",
    "    #Drop segments of data where change in acceleration is too small\n",
    "    std_dev = data[new_col].mean()\n",
    "    data.loc[data[new_col]<std_dev/20,\"smoothed\"] = -1\n",
    "    \n",
    "    #When acceleration is too low, drop everything in the window where it was too small.\n",
    "    data['smoothed'] = data['smoothed'].rolling(center=True, \n",
    "                                                window=int(rate), \n",
    "                                                min_periods=rate//2).min()\n",
    "    \n",
    "    # Drop first 5 seconds and last 5 seconds for good measure\n",
    "    data = data.loc[data['smoothed']>0,:]\n",
    "    if trim_ends:\n",
    "        return data.loc[rate*5:(data.shape[0]-rate*5),[\"A_tot\",\"Samplerate\", \"JointID\"]].reset_index()\n",
    "    else:\n",
    "        return data.loc[:,[\"A_tot\",\"Samplerate\", \"JointID\"]].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def clean_data(data, downsample=True):\n",
    "    #Generate a column that corresponds to the person and the activity\n",
    "    #Note that JointID//47 gives the activity and JointID%47 gives the person\n",
    "    data[\"JointID\"] = 47*data[\"ActivityID\"] + data[\"PersonID\"]\n",
    "    \n",
    "    #Drop \"microwalk\" activity values since they don't matter for our classification tests.\n",
    "    data = data[data.ActivityID<=7]\n",
    "    \n",
    "    #All columns have same type of activity, so we drop ActivityID\n",
    "    data.drop([\"SessionID\", \"TimeDelta\", \"DeviceID\", \"ActivityID\",\"PersonID\"], 1, inplace=True)\n",
    "    \n",
    "    if downsample:\n",
    "        #Downsample the data so that the sample rate is always 50\n",
    "        #First, separate data in to rate groups\n",
    "        rate_groups = data.groupby(\"Samplerate\")\n",
    "        rate_50 = rate_groups.get_group(50)\n",
    "        rate_400 = rate_groups.get_group(400)\n",
    "\n",
    "        #Now parse the data by individual and average each 8 consecutive observations\n",
    "        downsampled_groups = []\n",
    "        person_groups = rate_400.groupby(\"JointID\")\n",
    "        for person in person_groups.groups:\n",
    "            group = person_groups.get_group(person)\n",
    "            group[\"Samplerate\"] = 50\n",
    "            downsampled_groups.append(group.groupby(lambda x: x//8).mean())\n",
    "\n",
    "        #Now that each individual has been downsampled, join everything back together.\n",
    "        downsampled_400 = pd.concat(downsampled_groups)\n",
    "        data=pd.concat([downsampled_400, rate_50])\n",
    "        \n",
    "    \n",
    "    #We add a column for the total acceleration at each point\n",
    "    data['A_tot'] = np.sqrt(data[\"Ax\"]**2 + data[\"Ay\"]**2 + data[\"Az\"]**2)\n",
    "    \n",
    "#     grouped_data = data.groupby(\"JointID\")\n",
    "#     chunks = []\n",
    "#     for group in grouped_data.groups:\n",
    "#         temp = grouped_data.get_group(group)\n",
    "#         if temp.shape[0]>5000:\n",
    "#             chunk = get_rid_of_useless_data(temp, trim_ends=True)\n",
    "#         else:\n",
    "#             chunk = get_rid_of_useless_data(temp, trim_ends=False)\n",
    "#         chunks.append(chunk)\n",
    "        \n",
    "#     data = pd.concat(chunks)\n",
    "    \n",
    "    #Classify slow walk and fast walk as simply \"walking\"\n",
    "    #data.replace(to_replace={'ActivityID':{2:1, 3:1}},inplace=True)\n",
    "    \n",
    "    \n",
    "    return data.loc[:,[\"A_tot\",\"Samplerate\", \"JointID\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def fourier_smooth(data, cols):\n",
    "    # Select the 10% of coefficients corresponding to the lowest frequencies\n",
    "    # (The above strategy works well for data sampled 400 times/second, not sure about 50 times/second)\n",
    "    num_coefs = data.shape[0]//20\n",
    "    for col in cols:\n",
    "        # Take DFT\n",
    "        dft = fft(data[col])\n",
    "        \n",
    "        # Select low frequcney coeffs from start and end of DFT\n",
    "        start = dft[:num_coefs]\n",
    "        end = dft[-num_coefs:]\n",
    "        \n",
    "        # Set all other coefs to 0\n",
    "        dft = np.zeros_like(dft)\n",
    "        dft[:num_coefs] = start\n",
    "        dft[-num_coefs:] = end\n",
    "        \n",
    "        # Take inverse DFT to get smoothed signal\n",
    "        smoothed_signal = ifft(dft)\n",
    "        data[\"FourierSmooth_\"+col] = smoothed_signal\n",
    "        \n",
    "    return data\n",
    "\n",
    "def rolling_smooth(data, cols,  min_samples=4):\n",
    "    stdev = data.Samplerate[0]/10.\n",
    "    for col in cols:\n",
    "        data[\"RollSmooth_\"+col] = data[col].rolling(center=True, \n",
    "                                                    window=int(.4*data.loc[0,'Samplerate']), \n",
    "                                                    min_periods=min_samples, win_type='gaussian').mean(std=stdev)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def wavelet_smooth(data, cols, num_discard=4, wave_type='db4'):\n",
    "    for col in cols:\n",
    "        #Take wavelet transform and throw away high frequency coefficients\n",
    "        wavelets = pywt.wavedec(data[col], wave_type)\n",
    "        for i in xrange(-num_discard,0):\n",
    "            wavelets[i] = np.zeros_like(wavelets[i])\n",
    "        denoised_signal = pywt.waverec(wavelets,wave_type)\n",
    "        \n",
    "        try:\n",
    "            data[\"WavSmooth_\"+col] = denoised_signal\n",
    "        except:\n",
    "            data[\"WavSmooth_\"+col] = denoised_signal[:-1]\n",
    "    return data[\"WavSmooth_\"+col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def extract_gait_cycles(data, normalize=False, raw=False):\n",
    "    # Smooth the total acceleration to find cycle starting and ending points\n",
    "    data = data.copy().reset_index()\n",
    "    data = rolling_smooth(data,[\"A_tot\"])\n",
    "    rate = data.Samplerate[0]\n",
    "    \n",
    "    # Find \"salient\" minima and label them\n",
    "    data[\"Min_Salience\"] = data.loc[:,\"RollSmooth_A_tot\"].rolling(center=True, \n",
    "                                                                  window=int(1.2*rate), \n",
    "                                                                  min_periods=10).min()\n",
    "    data[\"salience_points\"] = (data.loc[:,\"Min_Salience\"] == data.loc[:,\"RollSmooth_A_tot\"]).astype(int)\n",
    "    \n",
    "    # Enumerate different cycles and extract them into a list\n",
    "    data[\"cycle_label\"] = data.salience_points.cumsum()\n",
    "    \n",
    "    if raw:\n",
    "        return data\n",
    "    \n",
    "    gait_cycles = data.groupby(\"cycle_label\")\n",
    "    cycles = [gait_cycles.get_group(grp)[\"A_tot\"].as_matrix().flatten() for grp in gait_cycles.groups][1:-1]\n",
    "    \n",
    "    # Get labels for our data\n",
    "    labels = np.array([data.loc[0,\"JointID\"]]*len(cycles))\n",
    "    \n",
    "    # Calculate potentially relevant features\n",
    "    mins = np.array([cycle.min() for cycle in cycles])\n",
    "    maxs = np.array([cycle.max() for cycle in cycles])\n",
    "    stds = np.array([np.std(cycle) for cycle in cycles])\n",
    "    means = np.array([cycle.mean() for cycle in cycles])\n",
    "    cycle_lengths = np.array([len(cycle) for cycle in cycles])\n",
    "                                                                                                                  \n",
    "                                                                                                                  \n",
    "    # Normalize features if desired\n",
    "    if normalize:\n",
    "        cycles = [(cycle-mini)/(maxi-mini)for cycle, mini, maxi in zip(cycles, mins, maxs)]\n",
    "    \n",
    "    # Get interpolating polynomials and get standard set of points for data\n",
    "    polynomials = [interp1d(np.linspace(0,1,len(cycle)),cycle) for cycle in cycles]\n",
    "    interp_points = np.array([poly(np.linspace(0,1,51)) for poly in polynomials])\n",
    "    \n",
    "    interp_df = pd.DataFrame(interp_points) \n",
    "    labels_df = pd.DataFrame(labels, columns=[\"Label\"])\n",
    "    feats_df = pd.DataFrame(np.array([mins, maxs, stds, means, cycle_lengths]).transpose(),\n",
    "                            columns=[\"Min\",\"Max\",\"Std\",\"Mean\",\"Cycle_Length\"])\n",
    "\n",
    "    mask = (feats_df[\"Cycle_Length\"]>rate*.8)&(feats_df[\"Cycle_Length\"]<rate*1.5)\n",
    "    interp_df = interp_df.loc[mask].reset_index(drop=True)\n",
    "    feats_df = feats_df[mask].reset_index(drop=True)\n",
    "    labels_df = labels_df[mask].reset_index(drop=True)\n",
    "    \n",
    "#     print labels_df.head(), feats_df.head(), interp_df.head()\n",
    "    # Trim down data frames with too many rows for balanced training set\n",
    "    if labels_df.shape[0]>150:\n",
    "        \n",
    "        rows = np.random.choice(labels_df.shape[0]-1, size=150, replace=False)\n",
    "        print rows.min()\n",
    "        print rows.max()\n",
    "        print interp_df.shape\n",
    "        \n",
    "        interp_df = interp_df.loc[rows]\n",
    "        feats_df = feats_df.loc[rows]\n",
    "        labels_df = labels_df.loc[rows]\n",
    "    \n",
    "    \n",
    "    return interp_df, labels_df, feats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "accel_data = clean_data(pd.read_csv('ProjectData.csv'))\n",
    "group1 = accel_data.groupby(\"JointID\").get_group(53)\n",
    "group2 = accel_data.groupby(\"JointID\").get_group(55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "data = extract_gait_cycles(group2, raw=True)\n",
    "new_dat = data.loc[(data['cycle_label']>200)&(data['cycle_label']<204),[\"A_tot\",\"cycle_label\",\"salience_points\",\"RollSmooth_A_tot\"]]\n",
    "start_indices = new_dat[new_dat['salience_points']==1].index.tolist()\n",
    "ax1 = new_dat[[\"A_tot\"]].plot(x=np.linspace(new_dat.index.min()/50, new_dat.index.max()/50,new_dat.shape[0]))\n",
    "plt.title(\"Gait Cycle Segmentation\", fontsize=24)\n",
    "plt.ylabel(\"Acceleration\", fontsize=18)\n",
    "plt.xlabel(\"Time (seconds)\", fontsize=18)\n",
    "iters=0\n",
    "for i in start_indices:\n",
    "    if iters==0:\n",
    "        ax1.axvline(x=i/50, color='r', linestyle='--', label=\"Cycle Starts\")\n",
    "    else:\n",
    "        ax1.axvline(x=i/50, color='r', linestyle='--')\n",
    "    iters += 1\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print data.columns\n",
    "data = extract_gait_cycles(group1, raw=True)\n",
    "new_dat = data.loc[(data['cycle_label']>200)&(data['cycle_label']<225),[\"A_tot\",\"cycle_label\",\"salience_points\",\"RollSmooth_A_tot\"]]\n",
    "start_indices = new_dat[new_dat['salience_points']==1].index.tolist()\n",
    "ax1 = new_dat[[\"A_tot\", \"RollSmooth_A_tot\"]].plot()\n",
    "for i in start_indices:\n",
    "    ax1.axvline(x=i, color='r', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print data.columns\n",
    "data = extract_gait_cycles(group1, raw=True)\n",
    "new_dat = data.loc[(data['cycle_label']>70)&(data['cycle_label']<76),[\"A_tot\",\"cycle_label\",\"salience_points\",\"RollSmooth_A_tot\"]]\n",
    "start_indices = new_dat[new_dat['salience_points']==1].index.tolist()\n",
    "ax1 = new_dat[[\"A_tot\"]].plot(x=np.linspace(new_dat.index.min()/50, new_dat.index.max()/50,new_dat.shape[0]))\n",
    "plt.title(\"Gait Cycle Segmentation\", fontsize=24)\n",
    "plt.ylabel(\"Acceleration\", fontsize=18)\n",
    "plt.xlabel(\"Time (seconds)\", fontsize=18)\n",
    "iters=0\n",
    "for i in start_indices:\n",
    "    if iters==0:\n",
    "        ax1.axvline(x=i/50, color='r', linestyle='--', label=\"Cycle Starts\")\n",
    "    else:\n",
    "        ax1.axvline(x=i/50, color='r', linestyle='--')\n",
    "    iters += 1\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def template_split(data, label_col):\n",
    "    #Randomly assigns rows to training and testing data\n",
    "    cols = data.columns\n",
    "    grouped_data = data.groupby(label_col)\n",
    "    train, test = [], []\n",
    "    for group in grouped_data.groups:\n",
    "        data = grouped_data.get_group(group).reset_index()\n",
    "        \n",
    "        num_rows = data.shape[0]\n",
    "\n",
    "        test_rows = np.random.choice(num_rows, size=np.floor(.3*num_rows), replace=False)\n",
    "        test_data = data.loc[test_rows]\n",
    "        train_mask = np.ones(num_rows, dtype=bool)\n",
    "        train_mask[test_rows] = 0\n",
    "        train_data = data.loc[train_mask]\n",
    "        test.append(test_data.loc[:,cols])\n",
    "        train.append(train_data.loc[:,cols])\n",
    "    test = pd.concat(test)\n",
    "    train = pd.concat(train)\n",
    "#     print train.columns\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "groups = accel_data.groupby(by=\"JointID\", as_index=False)\n",
    "data_groups = [groups.get_group(i) for i in groups.groups]\n",
    "\n",
    "# Extract gait cycles from data\n",
    "interp_pts = []\n",
    "labels = []\n",
    "other_feats = []\n",
    "for group in data_groups:\n",
    "    pts, labs, feats = extract_gait_cycles(group, normalize=False)\n",
    "    interp_pts.append(pts)\n",
    "    labels.append(labs)\n",
    "    other_feats.append(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Merge interpolated points together into single data frame\n",
    "interp_points = pd.concat(interp_pts).reset_index().loc[:,[i for i in range(51)]]\n",
    "\n",
    "new_labels = pd.concat(labels).reset_index().loc[:,[\"Label\"]]\n",
    "\n",
    "new_other_feats = pd.concat(other_feats).reset_index().loc[:,[\"Min\",\"Max\",\"Std\",\"Mean\",\"Cycle_Length\"]]\n",
    "interp_points = pd.concat([interp_points, new_labels],axis=1)\n",
    "full_feats = pd.concat([interp_points, new_other_feats], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Split data by train and test\n",
    "interp_train, interp_test = template_split(interp_points, label_col=\"Label\")\n",
    "full_train, full_test = template_split(full_feats, label_col=\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Split data by train and test\n",
    "interp_train, interp_test = template_split(interp_points, label_col=\"Label\")\n",
    "full_train, full_test = template_split(full_feats, label_col=\"Label\")\n",
    "one_norm_accuracies = []\n",
    "two_norm_accuracies = []\n",
    "for i in xrange(10):\n",
    "    interp_train, interp_test = template_split(interp_points, label_col=\"Label\")\n",
    "    uniform_KNN = KNeighborsClassifier(n_neighbors=1, n_jobs=-1, p=1).fit(interp_train.drop(\"Label\",1),\n",
    "                                                             interp_train[\"Label\"])\n",
    "    uniform_accuracy = accuracy_score(uniform_KNN.predict(interp_test.drop(\"Label\",1)),\n",
    "                                                             interp_test[\"Label\"])\n",
    "    \n",
    "    weighted_KNN = KNeighborsClassifier(n_neighbors=1, n_jobs=-1,\n",
    "                                         p=2).fit(interp_train.drop(\"Label\",1),\n",
    "                                                             interp_train.Label)\n",
    "    weighted_accuracy = accuracy_score(weighted_KNN.predict(interp_test.drop(\"Label\",1)),\n",
    "                                                             interp_test[\"Label\"])\n",
    "#     print \"{0} Neighbors, Uniform: {1}\".format(i, uniform_accuracy)\n",
    "#     print \"{0} Neighbors, Weighted: {1}\\n\".format(i, weighted_accuracy)\n",
    "    one_norm_accuracies.append(uniform_accuracy)\n",
    "    two_norm_accuracies.append(weighted_accuracy)\n",
    "print \"Mean KNN 1-norm Accuracy:\", np.mean(one_norm_accuracies)\n",
    "print \"Mean KNN 2-norm Accuracy:\", np.mean(two_norm_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def agg_accuracy(classifier=KNeighborsClassifier(n_neighbors=1, n_jobs=-1, p=1)):\n",
    "    total_acc = []\n",
    "    for i in xrange(50):\n",
    "        interp_train, interp_test = template_split(interp_points, label_col=\"Label\")\n",
    "        model = classifier.fit(interp_train.drop(\"Label\",1),interp_train[\"Label\"])\n",
    "\n",
    "\n",
    "        interp_test[\"one_pred\"] = model.predict(interp_test.drop(\"Label\",1))\n",
    "    #     interp_test[\"two_pred\"] = weighted_KNN.predict(interp_test.drop(\"Label\",1))\n",
    "\n",
    "        accuracies = []\n",
    "        grouped = interp_test.groupby(\"Label\")\n",
    "        for i in grouped.groups:\n",
    "            group = grouped.get_group(i)\n",
    "            n = min([group.shape[0], 11])\n",
    "            rows = np.random.choice(group.index, size=n, replace=False)\n",
    "            mode, count = stats.mode(group.loc[rows,\"one_pred\"])\n",
    "            if mode==group.Label.as_matrix()[0] and count/n>.5:\n",
    "                accuracies.append(1)\n",
    "            else:\n",
    "                accuracies.append(0)\n",
    "        total_acc.append(np.mean(accuracies))\n",
    "\n",
    "    print np.mean(total_acc)\n",
    "\n",
    "    #     print \"{0} Neighbors, Uniform: {1}\".format(i, uniform_accuracy)\n",
    "    #     print \"{0} Neighbors, Weighted: {1}\\n\".format(i, weighted_accuracy)\n",
    "\n",
    "print \"KNN Aggregated Accuracy\"\n",
    "agg_accuracy()\n",
    "\n",
    "print \"Random Forest Aggregated Accuracy\"\n",
    "agg_accuracy(RandomForestClassifier(n_estimators=100, n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "rf_accuracies = []\n",
    "for i in xrange(5):\n",
    "    rf_acc = random_forest_model(interp_train[\"Label\"],interp_train.drop(\"Label\",1),\n",
    "                    interp_test[\"Label\"], interp_test.drop(\"Label\",1),\n",
    "                    n_runs=5, plot=False, model_type=\"Person\")[0]\n",
    "    rf_accuracies.append(rf_acc)\n",
    "print np.mean(rf_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "XGB_model(interp_train[\"Label\"],interp_train.drop(\"Label\",1),\n",
    "                    interp_test[\"Label\"], interp_test.drop(\"Label\",1),\n",
    "                    n_runs=10, plot=False, model_type=\"Person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print interp_train.shape\n",
    "print interp_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "accel_data = clean_data(pd.read_csv('ProjectData.csv'))\n",
    "labels = accel_data.JointID.unique()\n",
    "labels=labels[labels>=60]\n",
    "labels\n",
    "grouped = accel_data.groupby(\"JointID\")\n",
    "for i in labels:\n",
    "    data = grouped.get_group(i)\n",
    "    data.A_tot.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "accel_data = clean_data(pd.read_csv('ProjectData.csv'))\n",
    "grouped = accel_data.groupby(\"JointID\")\n",
    "group1=grouped.get_group(53)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "group1.plot(x=np.linspace(group1.index.min()/50, group1.index.max()/50,group1.shape[0]), y=\"A_tot\", legend=False)\n",
    "plt.xlabel(\"Time (Seconds)\", fontsize=18)\n",
    "plt.ylabel(\"Acceleration\", fontsize=18)\n",
    "plt.title(\"Acceleration Readings over Time\", fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def feature_extraction(data, split_var=\"JointID\"):\n",
    "    #When performing activity classification, we still split our data based on person and activity, \n",
    "    # but then label based on activity\n",
    "    if split_var==\"ActivityID\":\n",
    "        grouped = data.groupby([\"JointID\"])\n",
    "    else:\n",
    "        grouped = data.groupby([split_var])\n",
    "    x_s = []\n",
    "    labels = []\n",
    "    \n",
    "    #Calculate histogram bins to use later for features\n",
    "    histogram_bins = np.linspace(0,1,11)\n",
    "       \n",
    "    for user, features in grouped:\n",
    "        print user\n",
    "        #Get feature attributes on 3 second samples from each individual\n",
    "        #We assume no overlap between samples\n",
    "        sample_rate = features[\"Samplerate\"].as_matrix()[0]\n",
    "        num_samples = features.count().as_matrix()[0]\n",
    "        N_3_sec = int(sample_rate*3)\n",
    "        \n",
    "        #Normalize data by centering each individual at 0. \n",
    "        #features[\"A_tot\"] -= features[\"A_tot\"].mean()\n",
    "        \n",
    "        #Extract as many full samples as we can\n",
    "        n_extracts = int(num_samples//N_3_sec)\n",
    "        for i in xrange(n_extracts):\n",
    "            attributes = []\n",
    "            \n",
    "            \n",
    "            labels.append(features[split_var].as_matrix()[0])\n",
    "\n",
    "            subset = features[N_3_sec*i:N_3_sec*(i+1)]\n",
    "\n",
    "            \n",
    "#             attributes.append(subset.JointID.as_matrix()[0])\n",
    "            attributes.append(subset[\"A_tot\"].mean())\n",
    "            attributes.append(subset[\"A_tot\"].std())\n",
    "            attributes.append(subset[\"A_tot\"].max())\n",
    "            attributes.append(subset[\"A_tot\"].min())\n",
    "            attributes.append(subset[\"A_tot\"].max() - subset[\"A_tot\"].min())\n",
    "            \n",
    "            #Calculate RMS of total acceleration of each sample\n",
    "            attributes.append(np.sqrt(np.sum(np.square(subset[\"A_tot\"])/N_3_sec)))\n",
    "            \n",
    "            #Calculate signal entropy\n",
    "            dft = fft(subset.A_tot)\n",
    "            power = np.abs(dft)**2/len(dft)\n",
    "            p_i = power/np.sum(power)\n",
    "            pse = -(np.sum(p_i*np.log(p_i)))\n",
    "            attributes.append(pse)\n",
    "            \n",
    "            #Calculate histogram distribution of of samples in dataset\n",
    "            normed = (subset[\"A_tot\"]-subset.A_tot.min())/(subset.A_tot.max()-subset.A_tot.min())\n",
    "            for k in np.histogram(normed, bins=histogram_bins)[0]:\n",
    "                attributes.append(k)\n",
    "                \n",
    "            #Calculate total number of sign changes in the subset\n",
    "            subset.A_tot = subset.A_tot-subset.A_tot.mean()\n",
    "            attributes.append(np.sum(np.abs((np.sign(subset[\"A_tot\"][:-1]).as_matrix() - np.sign(subset[\"A_tot\"][1:]).as_matrix())/2)))\n",
    "            \n",
    "            \n",
    "            x_s.append(attributes)\n",
    "\n",
    "    dat = pd.DataFrame(np.hstack((np.array(labels).reshape((len(labels),1)),np.array(x_s))),\n",
    "                       columns=[\"Label\", \"Mean\",\"Std\",\"Max\",\"Min\",\"Range\",\"RMS\",\"FDE\"]+[str(i) for i in range(10)]+[\"Sign\"])\n",
    "    return dat\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def split_train_test_data(labels, features):\n",
    "    #Randomly assigns rows to training and testing data\n",
    "    \n",
    "    num_rows = features.shape[0]\n",
    "\n",
    "    test_rows = np.random.choice(num_rows, size=np.floor(.3*num_rows), replace=False)\n",
    "    test_features = features[test_rows]\n",
    "    test_labels = labels[test_rows]\n",
    "    train_mask = np.ones(num_rows, dtype=bool)\n",
    "    train_mask[test_rows] = 0\n",
    "    train_features = features[train_mask]\n",
    "    train_labels = labels[train_mask]\n",
    "    return train_labels, train_features, test_labels, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def random_forest_model(tr_labels, tr_features, te_labels, te_features, n_runs=10, plot=True, model_type=\"Person\"):\n",
    "    correctly_classified = np.zeros(n_runs)\n",
    "    run_importances = np.zeros((n_runs,10))\n",
    "    activity_classification = np.zeros(7)\n",
    "    \n",
    "    for i in xrange(n_runs):\n",
    "        print i+1\n",
    "        #Train a random forest and predict the number correct\n",
    "        rf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "        rf.fit(tr_features, tr_labels)\n",
    "        predicted_labels = rf.predict(te_features)\n",
    "        correct_proportion = 1.-len(np.where(predicted_labels != te_labels)[0])/te_labels.shape[0]\n",
    "        print \"Correct Proportion:\", correct_proportion\n",
    "        print te_labels\n",
    "        print accuracy_score(predicted_labels, te_labels)\n",
    "        #print \"Proportion correctly classified: \", correct_proportion\n",
    "        correctly_classified[i] = correct_proportion\n",
    "\n",
    "        #Calculate feature importances\n",
    "        feat_importances = rf.feature_importances_\n",
    "        hist_importances = np.sum(feat_importances[8:-1])\n",
    "        importances = np.empty(10)\n",
    "        importances[:8] = feat_importances[:8]\n",
    "        importances[9] = hist_importances\n",
    "        importances[8] = feat_importances[9]\n",
    "        run_importances[i,:] = importances\n",
    "    \n",
    "    activity_classification /= n_runs\n",
    "    correct = np.mean(correctly_classified)\n",
    "    final_importances = np.mean(run_importances, axis=0)\n",
    "    print \"Proportion correctly classified: \", correct\n",
    "    \n",
    "    if plot==True:\n",
    "        pos = np.arange(len(final_importances))+.5\n",
    "        #plt.barh(pos, importances, color='c')\n",
    "        plt.xlabel(\"Feature Importance\", fontsize=18)\n",
    "        plt.ylabel(\"Feature\", fontsize=18)\n",
    "        plt.title(\"Random Forest Feature Importances\", fontsize=24)\n",
    "        labels = [\"Mean\", \"Standard Deviation\",'Min', 'Max',\n",
    "                  \"Value Range\",\"Root Mean Square\",\"Sign Changes\",\n",
    "                  \"Frequency Domain Entropy\",'Sign Changes',\"Histogram Distribution\"]\n",
    "        \n",
    "        s = pd.Series(final_importances, index=labels)\n",
    "        mycolors = ['b', 'g', 'r', '#FFA500','#800080','c', '#00FF7F', \"m\", '#FF7F50','y']\n",
    "        s.plot(kind='barh', color=mycolors)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    return correct, final_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def XGB_model(tr_labels, tr_features, te_labels, te_features, n_runs=10, plot=True, model_type=\"Person\"):\n",
    "    correctly_classified = np.zeros(n_runs)\n",
    "    run_importances = np.zeros((n_runs,9))\n",
    "    activity_classification = np.zeros(7)\n",
    "    \n",
    "    for i in xrange(n_runs):\n",
    "        print i+1\n",
    "        #Train a random forest and predict the number correct\n",
    "        rf = XGBClassifier(n_estimators=100)\n",
    "        rf.fit(tr_features, tr_labels)\n",
    "        predicted_labels = rf.predict(te_features)\n",
    "        correct_proportion = 1.-len(np.where(predicted_labels != te_labels)[0])/te_labels.shape[0]\n",
    "        #print \"Proportion correctly classified: \", correct_proportion\n",
    "        correctly_classified[i] = correct_proportion\n",
    "\n",
    "#         #Calculate feature importances\n",
    "#         feat_importances = rf.feature_importances_\n",
    "#         hist_importances = np.sum(feat_importances[8:])\n",
    "#         importances = np.empty(9)\n",
    "#         importances[:8] = feat_importances[:8]\n",
    "#         importances[8] = hist_importances\n",
    "#         run_importances[i,:] = importances\n",
    "    \n",
    "    activity_classification /= n_runs\n",
    "    correct = np.mean(correctly_classified)\n",
    "    final_importances = np.mean(run_importances, axis=0)\n",
    "    print \"Proportion correctly classified: \", correct\n",
    "    \n",
    "    if plot==True:\n",
    "        pos = np.arange(len(final_importances))+.5\n",
    "        #plt.barh(pos, importances, color='c')\n",
    "        plt.xlabel(\"Feature Importance\")\n",
    "        labels = [\"Mean\", \"Standard Deviation\",'Min', 'Max',\n",
    "                  \"Value Range\",\"Root Mean Square\",\"Sign Changes\",\n",
    "                  \"Frequency Domain Entropy\",\"Histogram Distribution\"]\n",
    "        \n",
    "        s = pd.Series(final_importances, index=labels)\n",
    "        mycolors = ['b', 'g', 'r', '#FFA500','#800080','c', '#00FF7F', \"m\", '#FF7F50']\n",
    "        \n",
    "        s.plot(kind='barh', color=mycolors)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    return correct, final_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def template_split(data, label_col=\"JointID\"):\n",
    "    #Randomly assigns rows to training and testing data\n",
    "    cols = data.columns\n",
    "    grouped_data = data.groupby(label_col)\n",
    "    train, test = [], []\n",
    "    for group in grouped_data.groups:\n",
    "        data = grouped_data.get_group(group).reset_index()\n",
    "        \n",
    "        num_rows = data.shape[0]\n",
    "\n",
    "        test_rows = np.random.choice(num_rows, size=np.floor(.3*num_rows), replace=False)\n",
    "        test_data = data.loc[test_rows]\n",
    "        train_mask = np.ones(num_rows, dtype=bool)\n",
    "        train_mask[test_rows] = 0\n",
    "        train_data = data.loc[train_mask]\n",
    "        test.append(test_data.loc[:,cols])\n",
    "        train.append(train_data.loc[:,cols])\n",
    "    test = pd.concat(test)\n",
    "    train = pd.concat(train)\n",
    "#     print train.columns\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "data = feature_extraction(accel_data, split_var=\"JointID\")\n",
    "print \"Features Created\"\n",
    "train, test = template_split(data, label_col=\"Label\")\n",
    "rain_l = train.Label\n",
    "train_f = train.drop(\"Label\",1)\n",
    "test_l = train.Label\n",
    "test_f = train.drop(\"Label\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "test.describe()\n",
    "np.unique(test.Label, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def person_model(train, test, find_tree_nums=False):\n",
    "    #Train a model that recognized people from only walking data\n",
    "    \n",
    "\n",
    "\n",
    "    train_l = train.Label\n",
    "    train_f = train.drop(\"Label\",1)\n",
    "    print train_f.head()\n",
    "    test_l = train.Label\n",
    "    test_f = train.drop(\"Label\",1)\n",
    "    print test_f.head()\n",
    "    print \"Data Split\"\n",
    "    if find_tree_nums:\n",
    "        print \"Running Optimal Tree Model\"\n",
    "        find_optimal_tree_num(train_l, train_f, test_l, test_f)\n",
    "    print \"Running Random Forest Trials\"\n",
    "    correct, feature_importances = random_forest_model(train_l, train_f, test_l, test_f, n_runs=10, plot=True)\n",
    "    \n",
    "person_model(train, test, find_tree_nums=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "person_model(find_tree_nums=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# full_data = clean_data(pd.read_csv('ProjectData.csv'), downsample=False)\n",
    "accel_data = clean_data(pd.read_csv('ProjectData.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "accel_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "full_group1 = full_data.groupby(by=\"JointID\", as_index=False).get_group(53)\n",
    "group1 = accel_data.groupby(by=\"JointID\", as_index=False).get_group(53)\n",
    "group1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "start_plot = 201000\n",
    "diff = 1200\n",
    "plt.title(\"Comparison of Regular and Downsampled Acceleration\")\n",
    "plt.ylabel(\"Acceleration\")\n",
    "plt.xlabel(\"Time (Seconds)\")\n",
    "plt.plot(np.linspace(0, diff/400, diff+1),full_group1.loc[start_plot:start_plot+diff, \"A_tot\"], label=\"Original Signal\")\n",
    "plt.plot(np.linspace(0, diff/400, diff//8+1),group1.loc[start_plot//8:(start_plot+diff)//8, \"A_tot\"], label=\"Downsampled Signal\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Extract gait cycles from each group in preparation for template based learning approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "labels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "group1.A_tot.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print len(groups.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "other_feats[0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "group1 = get_rid_of_useless_data(accel_data.groupby(by=\"JointID\", as_index=False).get_group(53))\n",
    "group1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print data.columns\n",
    "data = extract_gait_cycles(group1, raw=True)\n",
    "new_dat = data.loc[(data['cycle_label']>200)&(data['cycle_label']<205),[\"A_tot\",\"cycle_label\",\"salience_points\",\"RollSmooth_A_tot\"]]\n",
    "start_indices = new_dat[new_dat['salience_points']==1].index.tolist()\n",
    "ax1 = new_dat[[\"A_tot\", \"RollSmooth_A_tot\"]].plot()\n",
    "for i in start_indices:\n",
    "    ax1.axvline(x=i, color='r', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "group1 = accel_data.groupby(by=\"JointID\", as_index=False).get_group(53)\n",
    "data = extract_gait_cycles(group1, raw=True)\n",
    "new_dat = data.loc[(data['cycle_label']>212)&(data['cycle_label']<217),[\"A_tot\",\"cycle_label\",\"salience_points\",\"RollSmooth_A_tot\"]]\n",
    "start_indices = new_dat[new_dat['salience_points']==1].index.tolist()\n",
    "ax1 = new_dat[[\"A_tot\", \"RollSmooth_A_tot\"]].plot()\n",
    "for i in start_indices:\n",
    "    ax1.axvline(x=i, color='r', linestyle='--')\n",
    "plt.show()\n",
    "a,b,c = extract_gait_cycles(group1)\n",
    "c.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "group1 = accel_data.groupby(by=\"JointID\", as_index=False).get_group(53)\n",
    "data = extract_gait_cycles(group1, raw=True)\n",
    "new_dat = data.loc[(data['cycle_label']>212)&(data['cycle_label']<217),[\"A_tot\",\"cycle_label\",\"salience_points\",\"RollSmooth_A_tot\"]]\n",
    "start_indices = new_dat[new_dat['salience_points']==1].index.tolist()\n",
    "ax1 = new_dat[[\"A_tot\", \"RollSmooth_A_tot\"]].plot()\n",
    "for i in start_indices:\n",
    "    ax1.axvline(x=i, color='r', linestyle='--')\n",
    "plt.show()\n",
    "a,b,c = extract_gait_cycles(group1)\n",
    "print a.shape\n",
    "print b.shape\n",
    "print c.shape\n",
    "c.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gait_cycles = new_dat.groupby(\"cycle_label\")\n",
    "cycles = [gait_cycles.get_group(grp)[\"RollSmooth_A_tot\"].as_matrix().flatten() for grp in gait_cycles.groups]\n",
    "cycles = [(cycle-cycle.min())/(cycle.max()-cycle.min()) for cycle in cycles]\n",
    "polynomials = [interp1d(np.linspace(0,1,len(cycle)),cycle) for cycle in cycles]\n",
    "# for cycle in cycles[5:10]:\n",
    "#     plt.plot(np.linspace(0,1,len(cycle)),cycle)\n",
    "for poly in polynomials:\n",
    "    plt.plot(np.linspace(0,1,51), poly(np.linspace(0,1,51)), 'b-', alpha=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "roll_group = group1.rolling(center=True, window=int(.2*group1.Samplerate[1]), min_periods=10, win_type=\"gaussian\").mean(std=2)\n",
    "group1.loc[start_plot:start_plot+diff,\"A_tot\"].plot()\n",
    "roll_group.loc[start_plot:start_plot+diff,\"A_tot\"].plot(color='red')\n",
    "\n",
    "def rolling_smooth(data, cols, window_proportion=.1, min_samples=4):\n",
    "    stdev = data.Samplerate/100.\n",
    "    for col in cols:\n",
    "        data[\"RollSmooth_\"+col] = data[col].rolling(center=True, window=window_proportion*data.loc[0,'Samplerate'], \n",
    "                                                    min_periods=min_samples, win_type='gaussian').mean(std=stdev)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Figure out how to trim off data that doesn't actually contain walking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "\n",
    "\n",
    "dft = fft(group1.A_tot)\n",
    "# x_vals = np.arange(1,len(dft)+1).astype(float)\n",
    "# x_vals = x_vals*group1.Samplerate[1]/len(dft)\n",
    "num_coefs = group1.shape[0]//20\n",
    "start_plot = 10000\n",
    "diff = 2000\n",
    "start = dft[:num_coefs]\n",
    "end = dft[-num_coefs:]\n",
    "dft = np.zeros_like(dft)\n",
    "dft[:num_coefs] = start\n",
    "dft[-num_coefs:] = end\n",
    "# plt.plot(dft)\n",
    "# plt.show()\n",
    "smoothed_signal = ifft(dft)\n",
    "plt.plot(group1.loc[start_plot:start_plot+diff,\"A_tot\"].as_matrix())\n",
    "# plt.show()\n",
    "plt.plot(smoothed_signal[start_plot:start_plot+diff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Find Min Salience Vector as described in literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "roll_group = group1.rolling(center=True, window=int(.5*group1.Samplerate[1]), min_periods=10).min()\n",
    "roll_group.loc[200000:202000,\"A_tot\"].plot()\n",
    "group1.loc[200000:202000,\"A_tot\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "\n",
    "#     std_dev = data[col].std()\n",
    "#     new_col = col+\"_rolling_std\"\n",
    "#     data[new_col] = data[col].rolling(center=True, window=int(data.Samplerate[1]), min_periods=data.Samplerate[0]//2).std()\n",
    "#     data.loc[data[new_col]<std_dev/20,new_col] = -1\n",
    "#     data.loc[data[new_col]<0, col] = -10000\n",
    "\n",
    "#     data = data.copy()\n",
    "#     data[\"smoothed\"] = wavelet_smooth(data, [col])\n",
    "#     std_dev = data[\"smoothed\"].std()\n",
    "#     new_col = col+\"_rolling_std\"\n",
    "#     data[new_col] = data['smoothed'].rolling(center=True, window=int(data.Samplerate[1]), min_periods=data.Samplerate[0]//2).std()\n",
    "#     data.loc[data[new_col]<std_dev/15,\"smoothed\"] = -1\n",
    "#     data[new_col] = data['smoothed'].rolling(center=True, window=int(data.Samplerate[1]), min_periods=data.Samplerate[0]//2).min()\n",
    "#     data.loc[data[new_col]<0, col] = -10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "function_test = get_rid_of_useless_data(group1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "function_test[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "function_test.A_tot.plot()\n",
    "plt.show()\n",
    "group1.A_tot.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def segment_gait_cycles(data):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
